# core/tools/paper_toolkit/search_tool.py

import os
import json
import asyncio
import aiohttp
import hashlib
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any, TypedDict
from enum import Enum

from langchain_core.tools import tool
from pydantic import BaseModel, Field

from utils.logs import logger

# ==================== æ•°æ®æ¨¡å‹å®šä¹‰ ====================

class PaperMetadata(TypedDict):
    """è®ºæ–‡å…ƒæ•°æ®ç»“æ„"""
    title: str
    authors: List[str]
    abstract: str
    year: int
    venue: str
    citation_count: Optional[int]
    doi: Optional[str]
    url: str
    pdf_url: Optional[str]
    source: str
    is_survey: bool
    source_id: str
    relevance_score: float
    citations: List[str]  # å¼•ç”¨æ–‡çŒ®åˆ—è¡¨ï¼ˆå­˜å‚¨å¼•ç”¨è®ºæ–‡çš„æ ‡é¢˜ï¼‰ï¼Œé€šè¿‡å·¥å…·2è·å–


class SearchGoal(str, Enum):
    """æœç´¢ç›®æ ‡æšä¸¾"""
    FIND_SURVEYS = "find_surveys"
    FIND_EMPIRICAL = "find_empirical_studies"
    FIND_LATEST = "find_latest_advances"
    FIND_HIGHLY_CITED = "find_highly_cited"


class SearchPlan(BaseModel):
    """æœç´¢è®¡åˆ’"""
    primary_goal: SearchGoal
    secondary_goals: List[SearchGoal] = Field(default_factory=list)
    keywords: List[str] = Field(default_factory=list)
    time_filter: Optional[str] = None
    must_include: List[str] = Field(default_factory=list)
    exclude: List[str] = Field(default_factory=list)


# ==================== æ•°æ®æºé€‚é…å™¨ ====================

class BaseSearchAdapter:
    """æ•°æ®æºé€‚é…å™¨åŸºç±»"""

    def __init__(self, name: str, max_results: int = 50):
        self.name = name
        self.max_results = max_results

    def build_query(self, plan: SearchPlan) -> Any:
        """æ„å»ºæŸ¥è¯¢"""
        raise NotImplementedError

    async def search(self, plan: SearchPlan) -> List[PaperMetadata]:
        """æ‰§è¡Œæœç´¢"""
        raise NotImplementedError

    def _is_survey(self, title: str, abstract: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç»¼è¿°è®ºæ–‡"""
        survey_keywords = ['survey', 'review', 'overview', 'state of the art',
                           'comprehensive study', 'literature review']
        text = f"{title.lower()} {abstract.lower()}"
        return any(keyword in text for keyword in survey_keywords)

# ==================== Semantic Scholar é€‚é…å™¨ ====================

class SemanticScholarAdapter(BaseSearchAdapter):
    """Semantic Scholar é€‚é…å™¨"""

    def __init__(self):
        super().__init__("semantic_scholar")
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.api_key = os.getenv("SEMANTIC_SCHOLAR_API_KEY")

    def build_query(self, plan: SearchPlan) -> Dict[str, Any]:
        """æ„å»ºæŸ¥è¯¢å‚æ•°"""
        query_params = {
            "query": " ".join(plan.keywords),
            "limit": self.max_results,
            "fields": "title,authors,year,venue,abstract,citationCount,url,externalIds,openAccessPdf",
            "sort": "relevance" if plan.primary_goal == SearchGoal.FIND_SURVEYS else "citationCount:desc"
        }

        if plan.time_filter:
            current_year = datetime.now().year
            if plan.time_filter == "last_1_year":
                query_params["year"] = f"{current_year - 1}-{current_year}"
            elif plan.time_filter == "last_3_years":
                query_params["year"] = f"{current_year - 3}-{current_year}"
            elif plan.time_filter == "last_5_years":
                query_params["year"] = f"{current_year - 5}-{current_year}"

        return query_params

    async def search(self, plan: SearchPlan) -> List[PaperMetadata]:
        """æ‰§è¡Œæœç´¢"""
        query_params = self.build_query(plan)
        headers = {}
        if self.api_key:
            headers["x-api-key"] = self.api_key

        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(
                        f"{self.base_url}/paper/search",
                        params=query_params,
                        headers=headers,
                        timeout=30
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._parse_results(data.get("data", []))
                    else:
                        logger.error(f"Semantic Scholar API error: {response.status}")
                        return []
            except Exception as e:
                logger.error(f"Semantic Scholar search failed: {e}")
                return []

    def _parse_results(self, results: List[Dict]) -> List[PaperMetadata]:
        """è§£æç»“æœ"""
        papers = []

        for item in results:
            try:
                paper = PaperMetadata(
                    title=item.get("title", ""),
                    authors=[author.get("name", "") for author in item.get("authors", [])],
                    abstract=item.get("abstract", ""),
                    year=item.get("year", 0),
                    venue=item.get("venue", ""),
                    citation_count=item.get("citationCount", 0),
                    doi=item.get("externalIds", {}).get("DOI"),
                    url=item.get("url", ""),
                    pdf_url=item.get("openAccessPdf", {}).get("url"),
                    source=self.name,
                    is_survey=self._is_survey(
                        item.get("title", ""),
                        item.get("abstract", "")
                    ),
                    source_id=item.get("paperId", ""),
                    relevance_score=0.0,
                    citations=[]  # æœç´¢é˜¶æ®µå…ˆè®¾ä¸ºç©ºï¼Œåç»­ç”±å·¥å…·2å¡«å……
                )
                papers.append(paper)
            except Exception as e:
                logger.warning(f"Failed to parse paper: {e}")
                continue

        return papers

# ==================== arXiv é€‚é…å™¨ ====================

class ArxivAdapter(BaseSearchAdapter):
    """arXiv é€‚é…å™¨"""

    def __init__(self):
        super().__init__("arxiv")
        self.base_url = "http://export.arxiv.org/api/query"

    def build_query(self, plan: SearchPlan) -> Dict[str, Any]:
        """æ„å»ºæŸ¥è¯¢å‚æ•°"""
        query_parts = []

        if plan.keywords:
            query_parts.append(f"all:{' AND '.join(plan.keywords)}")

        if plan.time_filter:
            if plan.time_filter == "last_1_year":
                query_parts.append("submittedDate:[NOW-365DAYS TO NOW]")
            elif plan.time_filter == "last_3_years":
                query_parts.append("submittedDate:[NOW-1095DAYS TO NOW]")

        for term in plan.must_include:
            query_parts.append(f"abs:{term}")

        query_str = " AND ".join(query_parts) if query_parts else "all:*"

        return {
            "search_query": query_str,
            "start": 0,
            "max_results": self.max_results,
            "sortBy": "relevance",
            "sortOrder": "descending"
        }

    async def search(self, plan: SearchPlan) -> List[PaperMetadata]:
        """æ‰§è¡Œæœç´¢"""
        query_params = self.build_query(plan)

        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(
                        self.base_url,
                        params=query_params,
                        timeout=30
                ) as response:
                    if response.status == 200:
                        xml_content = await response.text()
                        return self._parse_results(xml_content)
                    else:
                        logger.error(f"arXiv API error: {response.status}")
                        return []
            except Exception as e:
                logger.error(f"arXiv search failed: {e}")
                return []

    def _parse_results(self, xml_content: str) -> List[PaperMetadata]:
        """è§£æç»“æœ"""
        import xml.etree.ElementTree as ET

        try:
            root = ET.fromstring(xml_content)
            namespace = {'atom': 'http://www.w3.org/2005/Atom'}

            papers = []
            for entry in root.findall('atom:entry', namespace):
                try:
                    title = entry.find('atom:title', namespace).text.strip()
                    summary = entry.find('atom:summary', namespace).text.strip() if entry.find('atom:summary',
                                                                                               namespace) is not None else ""

                    authors = []
                    for author_elem in entry.findall('atom:author', namespace):
                        name_elem = author_elem.find('atom:name', namespace)
                        if name_elem is not None:
                            authors.append(name_elem.text)

                    published = entry.find('atom:published', namespace).text
                    year = int(published[:4]) if published else 0

                    arxiv_id = entry.find('atom:id', namespace).text.split('/')[-1]

                    paper = PaperMetadata(
                        title=title,
                        authors=authors,
                        abstract=summary,
                        year=year,
                        venue="arXiv",
                        citation_count=None,
                        doi=None,
                        url=f"https://arxiv.org/abs/{arxiv_id}",
                        pdf_url=f"https://arxiv.org/pdf/{arxiv_id}.pdf",
                        source=self.name,
                        is_survey=self._is_survey(title, summary),
                        source_id=arxiv_id,
                        relevance_score=0.0,
                        citations=[]  # arXiv APIä¸æä¾›å¼•ç”¨åˆ—è¡¨ï¼Œåç»­ç”±å·¥å…·2è·å–
                    )
                    papers.append(paper)
                except Exception as e:
                    logger.warning(f"Failed to parse arXiv entry: {e}")
                    continue

            return papers
        except Exception as e:
            logger.error(f"Failed to parse arXiv XML: {e}")
            return []


# ==================== LLM å®¢æˆ·ç«¯ ====================

class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼Œé€šè¿‡llm-gatewayè°ƒç”¨"""

    def __init__(self):
        self.api_base = os.getenv("LLM_GATEWAY_URL", "http://localhost:4000")
        self.api_key = os.getenv("LLM_GATEWAY_API_KEY", "sk-local-dev")
        self.model = os.getenv("LLM_MODEL", "kimi")  # é»˜è®¤ä½¿ç”¨kimi

    async def acomplete(self, prompt: str, **kwargs) -> str:
        """å¼‚æ­¥è°ƒç”¨LLMå®Œæˆè¯·æ±‚"""
        import httpx

        url = f"{self.api_base}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": kwargs.get("temperature", 0.1),
            "max_tokens": kwargs.get("max_tokens", 1000)
        }

        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(url, headers=headers, json=data)
                response.raise_for_status()
                result = response.json()
                return result["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error(f"LLM Gateway error: {e}")
                raise


# ==================== æ··åˆæ’åºç®—æ³• ====================

class PaperRanker:
    """è®ºæ–‡æ’åºå™¨"""

    def __init__(self):
        self.weights = {
            'relevance': 0.4,
            'citation': 0.3,
            'recency': 0.2,
            'survey_boost': 0.5
        }

    def compute_relevance_score(self, paper: PaperMetadata, keywords: List[str]) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        text = f"{paper['title']} {paper['abstract']}".lower()
        score = 0.0

        for keyword in keywords:
            if keyword.lower() in text:
                keyword_lower = keyword.lower()
                title_score = paper['title'].lower().count(keyword_lower) * 2
                abstract_score = paper['abstract'].lower().count(keyword_lower) * 1
                score += (title_score + abstract_score)

        return min(score / 10.0, 1.0)

    def compute_recency_score(self, paper: PaperMetadata) -> float:
        """è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°"""
        current_year = datetime.now().year
        if paper['year'] == 0:
            return 0.5

        age = current_year - paper['year']
        if age <= 1:
            return 1.0
        elif age <= 3:
            return 0.8
        elif age <= 5:
            return 0.5
        else:
            return 0.2

    def compute_citation_score(self, paper: PaperMetadata) -> float:
        """è®¡ç®—å¼•ç”¨åˆ†æ•°"""
        if paper['citation_count'] is None:
            return 0.5

        if paper['citation_count'] == 0:
            return 0.1
        else:
            import math
            return min(math.log10(paper['citation_count'] + 1) / 3.0, 1.0)

    def compute_hybrid_score(self, paper: PaperMetadata, plan: SearchPlan) -> float:
        """è®¡ç®—æ··åˆæ’åºåˆ†æ•°"""
        relevance = self.compute_relevance_score(paper, plan.keywords)
        citation = self.compute_citation_score(paper)
        recency = self.compute_recency_score(paper)

        base_score = (
                self.weights['relevance'] * relevance +
                self.weights['citation'] * citation +
                self.weights['recency'] * recency
        )

        if paper['is_survey'] and SearchGoal.FIND_SURVEYS in [plan.primary_goal] + plan.secondary_goals:
            base_score += self.weights['survey_boost']

        return min(base_score, 1.0)


# ==================== æœç´¢å·¥å…· ====================

SEARCH_TOOL_DESCRIPTION = (
    "æ™ºèƒ½æœç´¢å­¦æœ¯è®ºæ–‡ï¼Œç‰¹åˆ«æ“…é•¿å®šä½é¢†åŸŸå†…çš„ç»¼è¿°è®ºæ–‡å’Œé«˜å½±å“åŠ›ç ”ç©¶ã€‚"
    "è¾“å…¥åº”ä¸ºè‡ªç„¶è¯­è¨€æè¿°çš„ç ”ç©¶é¢†åŸŸï¼ˆå¦‚'å¼ºåŒ–å­¦ä¹ çš„æœ€æ–°è¿›å±•'æˆ–'å°æ ·æœ¬å­¦ä¹ ç»¼è¿°'ï¼‰ã€‚"
    "å·¥å…·ä¼šè‡ªåŠ¨åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œå¹¶è¡Œæœç´¢å¤šä¸ªå­¦æœ¯æ•°æ®åº“ï¼Œå¹¶æ™ºèƒ½æ’åºç»“æœã€‚"
)


@tool(description=SEARCH_TOOL_DESCRIPTION)
async def research_paper_search(
        query: str,
        max_results: int = 20
) -> Dict[str, Any]:
    """
    æ™ºèƒ½æœç´¢å­¦æœ¯è®ºæ–‡ã€‚

    Args:
        query: å¯¹ç ”ç©¶é¢†åŸŸçš„è‡ªç„¶è¯­è¨€æè¿°
        max_results: æœŸæœ›è¿”å›çš„æœ€å¤§è®ºæ–‡æ•°é‡ï¼Œé»˜è®¤20

    Returns:
        åŒ…å«æœç´¢ç»“æœå’Œå…ƒä¿¡æ¯çš„å­—å…¸
    """
    # åˆå§‹åŒ–ç»„ä»¶
    llm_client = LLMClient()
    ranker = PaperRanker()

    # 1. æŸ¥è¯¢åˆ†æä¸è§„åˆ’
    logger.info(f"Analyzing query: {query}")
    search_plan = await _plan_search(query, llm_client)

    # 2. å¤šæºå¹¶è¡Œæ£€ç´¢
    logger.info(f"Searching with plan: {search_plan.dict()}")
    all_papers = await _search_all_sources(search_plan)

    # 3. å»é‡
    deduplicated_papers = _deduplicate_papers(all_papers)

    # 4. æ’åº
    for paper in deduplicated_papers:
        paper['relevance_score'] = ranker.compute_hybrid_score(paper, search_plan)

    # æ ¹æ®ä¸»è¦ç›®æ ‡è°ƒæ•´æ’åº
    if search_plan.primary_goal == SearchGoal.FIND_SURVEYS:
        deduplicated_papers.sort(key=lambda x: (not x['is_survey'], -x['relevance_score']))
    else:
        deduplicated_papers.sort(key=lambda x: -x['relevance_score'])

    ranked_papers = deduplicated_papers[:max_results]

    # 5. ç”Ÿæˆè§£é‡Š
    reasoning = await _generate_reasoning(query, search_plan, ranked_papers, llm_client)

    # 6. ç»Ÿè®¡æºè´¡çŒ®
    source_stats = _calculate_source_stats(ranked_papers)

    # 7. è¿”å›ç»“æœ
    return {
        "papers": ranked_papers,
        "search_strategy": search_plan.dict(),
        "reasoning": reasoning,
        "source_stats": source_stats,
        "query_time": datetime.now().isoformat(),
        "query": query
    }


async def _plan_search(query: str, llm_client: LLMClient) -> SearchPlan:
    """åˆ†ææŸ¥è¯¢å¹¶ç”Ÿæˆæœç´¢è®¡åˆ’"""
    planner_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ç ”ç©¶åŠ©ç†ã€‚è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶åˆ¶å®šä¸€ä¸ªç²¾å‡†çš„æ–‡çŒ®æœç´¢è®¡åˆ’ã€‚

    æŸ¥è¯¢ï¼š{query}

    è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    1. primary_goal: ä¸»è¦ç›®æ ‡ï¼Œå¯é€‰å€¼ï¼šfind_surveys, find_empirical_studies, find_latest_advances, find_highly_cited
    2. secondary_goals: æ¬¡è¦ç›®æ ‡åˆ—è¡¨ï¼Œå¯é€‰å€¼åŒä¸Š
    3. keywords: å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºæœç´¢
    4. time_filter: æ—¶é—´è¿‡æ»¤ï¼Œå¯é€‰å€¼ï¼šlast_1_year, last_3_years, last_5_years æˆ– null
    5. must_include: å¿…é¡»åŒ…å«çš„æœ¯è¯­åˆ—è¡¨
    6. exclude: æ’é™¤çš„æœ¯è¯­åˆ—è¡¨

    æ€è€ƒè¿‡ç¨‹ï¼š
    1. åˆ¤æ–­æŸ¥è¯¢æ„å›¾ï¼šæ˜¯å¯»æ±‚é¢†åŸŸæ¦‚è§ˆã€ç‰¹å®šæ–¹æ³•ã€æœ€æ–°è¿›å±•è¿˜æ˜¯ç»å…¸è®ºæ–‡ï¼Ÿ
    2. æå–æ ¸å¿ƒå­¦æœ¯æ¦‚å¿µã€æ–¹æ³•ã€æŠ€æœ¯åè¯
    3. åˆ¤æ–­æ—¶æ•ˆæ€§è¦æ±‚
    4. ç¡®å®šæ˜¯å¦éœ€è¦ç‰¹åˆ«å¼ºè°ƒç»¼è¿°è®ºæ–‡

    ç¤ºä¾‹è¾“å‡ºï¼š
    {{
        "primary_goal": "find_surveys",
        "secondary_goals": ["find_highly_cited"],
        "keywords": ["reinforcement learning", "exploration", "exploitation", "balance"],
        "time_filter": "last_3_years",
        "must_include": ["survey", "review"],
        "exclude": ["biology", "chemistry"]
    }}
    """

    try:
        response = await llm_client.acomplete(
            prompt=planner_prompt,
            temperature=0.1,
            max_tokens=500
        )

        # æå–JSONéƒ¨åˆ†
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            plan_dict = json.loads(json_match.group())
            return SearchPlan(**plan_dict)
        else:
            logger.warning("Failed to parse LLM response as JSON, using fallback")
            return _create_fallback_plan(query)

    except Exception as e:
        logger.error(f"Search planning failed: {e}")
        return _create_fallback_plan(query)


def _create_fallback_plan(query: str) -> SearchPlan:
    """åˆ›å»ºå¤‡ç”¨æœç´¢è®¡åˆ’"""
    words = query.lower().split()
    keywords = [w for w in words if len(w) > 3 and w not in ['the', 'and', 'for', 'with', 'about']]

    survey_indicators = ['survey', 'review', 'overview', 'introduction', 'state of the art']
    is_survey_query = any(indicator in query.lower() for indicator in survey_indicators)

    return SearchPlan(
        primary_goal=SearchGoal.FIND_SURVEYS if is_survey_query else SearchGoal.FIND_EMPIRICAL,
        keywords=keywords[:5],
        time_filter="last_3_years"
    )


async def _search_all_sources(plan: SearchPlan) -> List[PaperMetadata]:
    """å¹¶å‘æœç´¢æ‰€æœ‰æ•°æ®æº"""
    # åˆå§‹åŒ–é€‚é…å™¨
    adapters = []

    # æ ¹æ®é…ç½®å†³å®šå¯ç”¨å“ªäº›æ•°æ®æº
    enable_semantic_scholar = os.getenv("ENABLE_SEMANTIC_SCHOLAR", "true").lower() == "true"
    enable_arxiv = os.getenv("ENABLE_ARXIV", "true").lower() == "true"

    if enable_semantic_scholar:
        adapters.append(SemanticScholarAdapter())

    if enable_arxiv:
        adapters.append(ArxivAdapter())

    if not adapters:
        logger.warning("No search sources enabled!")
        return []

    # å¹¶å‘æœç´¢
    tasks = [adapter.search(plan) for adapter in adapters]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # åˆå¹¶ç»“æœ
    all_papers = []
    for i, result in enumerate(results):
        adapter_name = adapters[i].name if i < len(adapters) else "unknown"
        if isinstance(result, Exception):
            logger.error(f"Source {adapter_name} failed: {result}")
            continue
        all_papers.extend(result)

    logger.info(f"Retrieved {len(all_papers)} papers from {len(adapters)} sources")
    return all_papers


def _deduplicate_papers(papers: List[PaperMetadata]) -> List[PaperMetadata]:
    """è®ºæ–‡å»é‡"""
    seen = set()
    unique_papers = []

    for paper in papers:
        if paper['title'] and paper['authors']:
            title_hash = hashlib.md5(paper['title'].lower().encode()).hexdigest()
            first_author = paper['authors'][0].lower() if paper['authors'] else ""
            paper_hash = f"{title_hash}_{first_author}"

            if paper_hash not in seen:
                seen.add(paper_hash)
                unique_papers.append(paper)

    logger.info(f"Deduplicated: {len(papers)} -> {len(unique_papers)}")
    return unique_papers


async def _generate_reasoning(
        query: str,
        plan: SearchPlan,
        papers: List[PaperMetadata],
        llm_client: LLMClient
) -> str:
    """ç”Ÿæˆæœç´¢è§£é‡Š"""
    if not papers:
        return "æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚è¯·å°è¯•è°ƒæ•´æŸ¥è¯¢è¯æˆ–æ”¾å®½æœç´¢æ¡ä»¶ã€‚"

    top_titles = [p['title'] for p in papers[:3]]

    reasoning_prompt = f"""
    åŸºäºä»¥ä¸‹æœç´¢ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€æ®µç®€æ´ã€ä¸“ä¸šçš„æœç´¢è¿‡ç¨‹è§£é‡Šï¼š

    åŸå§‹æŸ¥è¯¢ï¼š{query}
    æœç´¢ç­–ç•¥ï¼š{plan.dict()}
    è¿”å›è®ºæ–‡æ•°é‡ï¼š{len(papers)}ç¯‡
    ä»£è¡¨æ€§è®ºæ–‡ï¼š
    {chr(10).join([f'- {title}' for title in top_titles])}

    è¯·ç”Ÿæˆä¸€æ®µ2-3å¥è¯çš„è§£é‡Šï¼Œè¯´æ˜ï¼š
    1. æœç´¢çš„é‡ç‚¹ï¼ˆå¦‚æ˜¯å¦ä¾§é‡ç»¼è¿°ã€æ—¶æ•ˆæ€§ã€é«˜å½±å“åŠ›ç­‰ï¼‰
    2. è¿”å›ç»“æœçš„ä¸»è¦ç‰¹ç‚¹
    3. ä»»ä½•éœ€è¦æ³¨æ„çš„äº‹é¡¹ï¼ˆå¦‚æŸäº›é¢†åŸŸè¦†ç›–æœ‰é™ï¼‰

    ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­æ°”ã€‚
    """

    try:
        response = await llm_client.acomplete(
            prompt=reasoning_prompt,
            temperature=0.3,
            max_tokens=300
        )
        return response.strip()
    except Exception as e:
        logger.error(f"Failed to generate reasoning: {e}")
        return "æœç´¢å·²å®Œæˆã€‚"


def _calculate_source_stats(papers: List[PaperMetadata]) -> Dict[str, int]:
    """è®¡ç®—æºè´¡çŒ®ç»Ÿè®¡"""
    stats = {}
    for paper in papers:
        source = paper['source']
        stats[source] = stats.get(source, 0) + 1
    return stats


# ==================== å·¥å…·å·¥å‚å‡½æ•° ====================

def get_paper_search_tool():
    """è·å–è®ºæ–‡æœç´¢å·¥å…·å®ä¾‹"""
    return research_paper_search


# ==================== æµ‹è¯•ä»£ç  ====================

async def test_search():
    """æµ‹è¯•æœç´¢å·¥å…·"""
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
    os.environ["LLM_GATEWAY_URL"] = "http://localhost:4000"
    os.environ["LLM_GATEWAY_API_KEY"] = "sk-local-dev"
    os.environ["LLM_MODEL"] = "kimi"

    print("ğŸ” æµ‹è¯•å­¦æœ¯è®ºæ–‡æœç´¢å·¥å…·...")

    # æ¨¡æ‹ŸLLMå“åº”
    async def mock_plan_search(query, llm_client):
        return SearchPlan(
            primary_goal=SearchGoal.FIND_SURVEYS,
            keywords=["reinforcement learning", "deep learning"],
            time_filter="last_3_years"
        )

    # ä¸´æ—¶æ›¿æ¢å‡½æ•°è¿›è¡Œæµ‹è¯•
    original_plan_search = _plan_search
    import core.tools.paper_toolkit.search_tool as module
    module._plan_search = mock_plan_search

    try:
        result = await research_paper_search(
            query="å¼ºåŒ–å­¦ä¹ çš„æœ€æ–°ç»¼è¿°",
            max_results=5
        )

        print(f"ğŸ“Š æ‰¾åˆ°è®ºæ–‡æ•°é‡: {len(result['papers'])}")
        print(f"ğŸ¯ æœç´¢ç­–ç•¥: {result['search_strategy']}")
        print(f"ğŸ’¡ è§£é‡Šè¯´æ˜: {result['reasoning']}")

        if result['papers']:
            print("\nğŸ“„ å‰3ç¯‡è®ºæ–‡:")
            for i, paper in enumerate(result['papers'][:3], 1):
                print(f"{i}. {paper['title'][:80]}...")
                print(f"   ä½œè€…: {', '.join(paper['authors'][:2])}")
                print(f"   å¹´ä»½: {paper['year']}, æ¥æº: {paper['source']}")
                print(f"   ç»¼è¿°: {paper['is_survey']}, åˆ†æ•°: {paper['relevance_score']:.3f}")
                print(f"   å¼•ç”¨æ•°: {len(paper['citations'])}")
                print()
    finally:
        # æ¢å¤åŸå‡½æ•°
        module._plan_search = original_plan_search


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    import asyncio
    asyncio.run(test_search())